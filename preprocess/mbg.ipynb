{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Detection of Mosquito Breeding Grounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide MBG videos into frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# def read_annotations(xml_file):\n",
    "#     tree = ET.parse(xml_file)\n",
    "#     root = tree.getroot()\n",
    "#     annotations = {}\n",
    "\n",
    "#     for track in root.findall(\".//track\"):\n",
    "#         label = track.attrib[\"label\"]\n",
    "\n",
    "#         for box in track.findall(\".//box\"):\n",
    "#             frame_number = int(box.attrib[\"frame\"])\n",
    "#             xmin = int(float(box.attrib[\"xtl\"]))\n",
    "#             ymin = int(float(box.attrib[\"ytl\"]))\n",
    "#             xmax = int(float(box.attrib[\"xbr\"]))\n",
    "#             ymax = int(float(box.attrib[\"ybr\"]))\n",
    "\n",
    "#             if frame_number not in annotations:\n",
    "#                 annotations[frame_number] = []\n",
    "\n",
    "#             annotations[frame_number].append({\n",
    "#                 'label': label,\n",
    "#                 'bbox': (xmin, ymin, xmax, ymax)\n",
    "#             })\n",
    "\n",
    "#     return annotations\n",
    "\n",
    "# def convert_to_yolo_format(class_id, image_width, image_height, bbox):\n",
    "#     x_center = (bbox[0] + bbox[2]) / 2 / image_width\n",
    "#     y_center = (bbox[1] + bbox[3]) / 2 / image_height\n",
    "#     width = (bbox[2] - bbox[0]) / image_width\n",
    "#     height = (bbox[3] - bbox[1]) / image_height\n",
    "\n",
    "#     return f\"{class_id} {x_center} {y_center} {width} {height}\"\n",
    "\n",
    "# def split_video(video_path, xml_file, output_image_directory, output_annotation_directory, frame_rate=1):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     annotations = read_annotations(xml_file)\n",
    "\n",
    "#     frame_number = 0\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         if frame_number in annotations:\n",
    "#             image_height, image_width, _ = frame.shape\n",
    "\n",
    "#             # Save the frame as an image\n",
    "#             output_image_file = os.path.join(output_image_directory, f\"frame_{frame_number:04d}.jpg\")\n",
    "#             cv2.imwrite(output_image_file, frame)\n",
    "\n",
    "#             # Save YOLO format annotation to file\n",
    "#             annotation_file_path = os.path.join(output_annotation_directory, f\"frame_{frame_number:04d}.txt\")\n",
    "#             with open(annotation_file_path, 'w') as yolo_file:\n",
    "#                 for annotation in annotations[frame_number]:\n",
    "#                     xmin, ymin, xmax, ymax = annotation['bbox']\n",
    "#                     label = annotation['label']\n",
    "\n",
    "#                     # Get class_id from the class_ids dictionary\n",
    "#                     class_id = class_ids.get(label)\n",
    "#                     if class_id is not None:\n",
    "#                         yolo_format = convert_to_yolo_format(class_id, image_width, image_height, (xmin, ymin, xmax, ymax))\n",
    "#                         yolo_file.write(f\"{yolo_format}\\n\")\n",
    "\n",
    "#         # Create the output directories if they don't exist\n",
    "#         os.makedirs(output_image_directory, exist_ok=True)\n",
    "#         os.makedirs(output_annotation_directory, exist_ok=True)\n",
    "\n",
    "#         # Save the frame if it's within the desired frame rate\n",
    "#         if frame_number % int(cap.get(cv2.CAP_PROP_FPS) / frame_rate) == 0:\n",
    "#             frame_number += 1\n",
    "#             continue\n",
    "\n",
    "#         frame_number += 1\n",
    "\n",
    "#     cap.release()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     video_directory = \"C:/Users/user/Downloads/FYP/fypfiles/mbgdataset/flightavi\"\n",
    "#     annotation_directory = \"C:/Users/user/Downloads/FYP/fypfiles/mbgdataset/flightann\"\n",
    "#     output_image_directory = \"C:/Users/user/Downloads/projects/MBGprocess/imageframes\"\n",
    "#     output_annotation_directory = \"C:/Users/user/Downloads/projects/MBGprocess/labelframes\"\n",
    "#     frame_rate = 1\n",
    "\n",
    "#     class_ids = {\"tire\": 0, \"bottle\": 1, \"bucket\": 2, \"watertank\": 3, \"pool\": 4, \"puddle\": 5}\n",
    "\n",
    "#     video_files = [f for f in os.listdir(video_directory) if f.endswith(\".avi\")]\n",
    "\n",
    "#     for video_file in video_files:\n",
    "#         video_path = os.path.join(video_directory, video_file)\n",
    "#         xml_file = os.path.join(annotation_directory, f\"{os.path.splitext(video_file)[0]}.xml\")\n",
    "#         split_video(video_path, xml_file, output_image_directory, output_annotation_directory, frame_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train, test, val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Set the path to the dataset directory\n",
    "# dataset_directory = \"C:/Users/user/Downloads/projects/MBGprocess/\"\n",
    "\n",
    "# # Set the paths for image and label folders\n",
    "# image_folder = os.path.join(dataset_directory, \"imageframes\")\n",
    "# label_folder = os.path.join(dataset_directory, \"labelframes\")\n",
    "\n",
    "# # Get lists of image and label files\n",
    "# image_files = os.listdir(image_folder)\n",
    "# label_files = os.listdir(label_folder)\n",
    "\n",
    "# # Ensure the lists are sorted for consistency\n",
    "# image_files.sort()\n",
    "# label_files.sort()\n",
    "\n",
    "# # Split the data into training, validation, and test sets\n",
    "# image_train, image_temp, label_train, label_temp = train_test_split(image_files, label_files, test_size=0.4, random_state=1)\n",
    "# image_val, image_test, label_val, label_test = train_test_split(image_temp, label_temp, test_size=0.5, random_state=1)\n",
    "\n",
    "# # First split to get 20% as test set\n",
    "# image_train, image_test, label_train, label_test = train_test_split(image_files, label_files, test_size=0.2, random_state=1)\n",
    "# # Split the training set again to get the validation set (requires calculation to get the needed percentage)\n",
    "# image_train, image_val, label_train, label_val = train_test_split(image_train, label_train, test_size=0.25, random_state=1)\n",
    "\n",
    "# # Display the lengths of the sets\n",
    "# print(\"Train set size (60%):\", len(image_train))\n",
    "# print(\"Validation set size (20%):\", len(image_val))\n",
    "# print(\"Test set size (20%):\", len(image_test))\n",
    "\n",
    "# # Now you have variables containing the file paths for each set\n",
    "# train_set = list(zip(image_train, label_train))\n",
    "# val_set = list(zip(image_val, label_val))\n",
    "# test_set = list(zip(image_test, label_test))\n",
    "\n",
    "# # Visualize the distribution of object classes\n",
    "# def visualize_class_distribution(labels):\n",
    "#     class_counts = {}\n",
    "#     for label_file in labels:\n",
    "#         with open(os.path.join(label_folder, label_file), 'r') as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in lines:\n",
    "#                 class_id = line.split()[0]\n",
    "#                 if class_id not in class_counts:\n",
    "#                     class_counts[class_id] = 1\n",
    "#                 else:\n",
    "#                     class_counts[class_id] += 1\n",
    "\n",
    "#     class_ids = sorted(list(map(int, class_counts.keys())))\n",
    "#     counts = [class_counts[str(class_id)] for class_id in class_ids]\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.bar(class_ids, counts, color='blue')\n",
    "#     plt.xlabel('Class ID')\n",
    "#     plt.ylabel('Number of Objects')\n",
    "#     plt.title('Distribution of Object Classes')\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize class distribution for training set\n",
    "# visualize_class_distribution(label_train)\n",
    "\n",
    "\n",
    "# #     class_ids = {\"tire\": 0, \"bottle\": 1, \"bucket\": 2, \"watertank\": 3, \"pool\": 4, \"puddle\": 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imgaug in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (0.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from imgaug) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from imgaug) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from imgaug) (1.11.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from imgaug) (9.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from imgaug) (3.6.3)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from imgaug) (0.22.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from imgaug) (4.8.1.78)\n",
      "Requirement already satisfied: imageio in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from imgaug) (2.33.1)\n",
      "Requirement already satisfied: Shapely in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from imgaug) (2.0.2)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image>=0.14.2->imgaug) (3.2.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image>=0.14.2->imgaug) (2023.12.9)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image>=0.14.2->imgaug) (23.1)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image>=0.14.2->imgaug) (0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->imgaug) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->imgaug) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->imgaug) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->imgaug) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->imgaug) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->imgaug) (2.8.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to the dataset directory\n",
    "dataset_directory = \"C:/Users/user/Downloads/projects/MBGprocess/\"\n",
    "\n",
    "# Set the paths for image and label folders\n",
    "image_folder = os.path.join(dataset_directory, \"imageframes\")\n",
    "label_folder = os.path.join(dataset_directory, \"labelframes\")\n",
    "\n",
    "# Get lists of image and label files\n",
    "image_files = os.listdir(image_folder)\n",
    "label_files = os.listdir(label_folder)\n",
    "\n",
    "# Ensure the lists are sorted for consistency\n",
    "image_files.sort()\n",
    "label_files.sort()\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "image_train_val, image_test, label_train_val, label_test = train_test_split(\n",
    "    image_files, label_files, test_size=0.2, random_state=1\n",
    ")\n",
    "image_train, image_val, label_train, label_val = train_test_split(\n",
    "    image_train_val, label_train_val, test_size=0.25, random_state=1\n",
    ")\n",
    "\n",
    "# Display the lengths of the sets\n",
    "print(\"Train set size:\", len(image_train))\n",
    "print(\"Validation set size:\", len(image_val))\n",
    "print(\"Test set size:\", len(image_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define augmentation pipeline\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),  # horizontal flips\n",
    "    iaa.Flipud(0.5),  # vertical flips\n",
    "    iaa.SomeOf((0, 5),\n",
    "               [\n",
    "                   iaa.Affine(rotate=(-45, 45)),\n",
    "                   iaa.Multiply((0.5, 1.5)),\n",
    "                   iaa.GaussianBlur(sigma=(0.0, 2.0)),\n",
    "                   iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "               ],\n",
    "               random_order=True\n",
    "               ),\n",
    "    # Adding cutout augmentation\n",
    "    iaa.Cutout(nb_iterations=1, size=0.2, squared=False, cval=(0, 255))\n",
    "])\n",
    "\n",
    "# Function to apply augmentation to an image and its bounding boxes\n",
    "def augment_data(image_path, label_path):\n",
    "    image = cv2.imread(os.path.join(image_folder, image_path))\n",
    "    bboxes = []\n",
    "    \n",
    "    with open(os.path.join(label_folder, label_path), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # Assuming YOLO format: class x_center y_center width height\n",
    "            class_id, x_center, y_center, width, height = map(float, line.split()[0:5])\n",
    "            \n",
    "            # Convert YOLO format to xmin, ymin, xmax, ymax\n",
    "            xmin = int((x_center - width / 2) * image.shape[1])\n",
    "            ymin = int((y_center - height / 2) * image.shape[0])\n",
    "            xmax = int((x_center + width / 2) * image.shape[1])\n",
    "            ymax = int((y_center + height / 2) * image.shape[0])\n",
    "            \n",
    "            bboxes.append(ia.BoundingBox(x1=xmin, y1=ymin, x2=xmax, y2=ymax, label=int(class_id)))\n",
    "    \n",
    "    bbs = ia.BoundingBoxesOnImage(bboxes, shape=image.shape)\n",
    "    \n",
    "    # Apply augmentation\n",
    "    augmented_image, augmented_bbs = seq(image=image, bounding_boxes=bbs)\n",
    "    \n",
    "    # Convert augmented bounding boxes back to YOLO format\n",
    "    augmented_labels = []\n",
    "    for bb in augmented_bbs.bounding_boxes:\n",
    "        class_id = bb.label\n",
    "        x_center = (bb.x1 + bb.x2) / 2 / augmented_image.shape[1]\n",
    "        y_center = (bb.y1 + bb.y2) / 2 / augmented_image.shape[0]\n",
    "        width = (bb.x2 - bb.x1) / augmented_image.shape[1]\n",
    "        height = (bb.y2 - bb.y1) / augmented_image.shape[0]\n",
    "        augmented_labels.append(f\"{int(class_id)} {x_center} {y_center} {width} {height}\\n\")\n",
    "    \n",
    "    return augmented_image, augmented_labels\n",
    "\n",
    "# Example: Apply augmentation to the first image and its labels in the training set\n",
    "image_path_train = image_train[0]\n",
    "label_path_train = label_train[0]\n",
    "augmented_image_train, _ = augment_data(image_path_train, label_path_train)\n",
    "\n",
    "# Display the original and augmented images\n",
    "visualize_images(\n",
    "    cv2.imread(os.path.join(image_folder, image_path_train)),\n",
    "    augmented_image_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to oversample minority classes by generating synthetic data\n",
    "def oversample_minority_classes(image_paths, label_paths, desired_class_balance, oversampling_factor=2):\n",
    "    class_distribution = get_class_distribution(label_paths)\n",
    "    \n",
    "    # Calculate the number of samples needed for each class\n",
    "    max_class_count = max(class_distribution.values())\n",
    "    oversampled_images = []\n",
    "    oversampled_labels = []\n",
    "\n",
    "    for class_id, count in class_distribution.items():\n",
    "        oversampling_required = max_class_count - count\n",
    "        oversampling_required *= oversampling_factor\n",
    "        \n",
    "        if oversampling_required > 0 and class_id not in desired_class_balance:\n",
    "            # Oversample the minority class\n",
    "            minority_samples = random.sample([(img, lbl) for img, lbl in zip(image_paths, label_paths) if int(lbl.split()[0]) == class_id], oversampling_required)\n",
    "            oversampled_images.extend(minority_samples)\n",
    "    \n",
    "    return oversampled_images\n",
    "\n",
    "# Define the desired class balance\n",
    "desired_class_balance = {0: 200, 1: 200, 2: 200}  # Adjust the class IDs and desired counts\n",
    "\n",
    "# Oversample the minority classes\n",
    "oversampled_train_set = oversample_minority_classes(image_train, label_train, desired_class_balance)\n",
    "\n",
    "# Visualize class distribution before and after oversampling\n",
    "class_distribution_before_oversampling = get_class_distribution(label_train)\n",
    "class_distribution_after_oversampling = get_class_distribution([lbl_file for _, lbl_file in oversampled_train_set])\n",
    "\n",
    "# Combine class distributions for original and oversampled sets\n",
    "combined_class_distribution_oversampled = {}\n",
    "for class_id in set(class_distribution_before_oversampling.keys()) | set(class_distribution_after_oversampling.keys()):\n",
    "    combined_class_distribution_oversampled[class_id] = class_distribution_before_oversampling.get(class_id, 0) + class_distribution_after_oversampling.get(class_id, 0)\n",
    "\n",
    "# Visualize class distribution before and after oversampling\n",
    "plt.bar(class_distribution_before_oversampling.keys(), class_distribution_before_oversampling.values(), color='blue', label='Original Training Set')\n",
    "plt.bar(class_distribution_after_oversampling.keys(), class_distribution_after_oversampling.values(), color='orange', label='Oversampled Training Set')\n",
    "\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Number of Objects')\n",
    "plt.title('Class Distribution in Original and Oversampled Training Sets')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'yolov5'\n",
      "c:\\Users\\user\\Downloads\\projects\\MBG-YOLOv7\\preprocess\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt comet_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and val folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directing splitted images into their belonged folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'c:\\\\Users\\\\user\\\\Downloads\\\\projects\\\\MBG-YOLOv7\\\\preprocess\\\\train.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s on MBG dataset\n",
    "!python train.py --img 512 --batch 32 --epochs 30 --data mbg_test.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'c:\\\\Users\\\\user\\\\Downloads\\\\projects\\\\MBG-YOLOv7\\\\preprocess\\\\detect.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --source runs/train/exp/test1.jpg --weights best.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
