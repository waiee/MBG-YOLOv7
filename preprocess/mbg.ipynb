{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Detection of Mosquito Breeding Grounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide MBG videos into frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# def read_annotations(xml_file):\n",
    "#     tree = ET.parse(xml_file)\n",
    "#     root = tree.getroot()\n",
    "#     annotations = {}\n",
    "\n",
    "#     for track in root.findall(\".//track\"):\n",
    "#         label = track.attrib[\"label\"]\n",
    "\n",
    "#         for box in track.findall(\".//box\"):\n",
    "#             frame_number = int(box.attrib[\"frame\"])\n",
    "#             xmin = int(float(box.attrib[\"xtl\"]))\n",
    "#             ymin = int(float(box.attrib[\"ytl\"]))\n",
    "#             xmax = int(float(box.attrib[\"xbr\"]))\n",
    "#             ymax = int(float(box.attrib[\"ybr\"]))\n",
    "\n",
    "#             if frame_number not in annotations:\n",
    "#                 annotations[frame_number] = []\n",
    "\n",
    "#             annotations[frame_number].append({\n",
    "#                 'label': label,\n",
    "#                 'bbox': (xmin, ymin, xmax, ymax)\n",
    "#             })\n",
    "\n",
    "#     return annotations\n",
    "\n",
    "# def convert_to_yolo_format(class_id, image_width, image_height, bbox):\n",
    "#     x_center = (bbox[0] + bbox[2]) / 2 / image_width\n",
    "#     y_center = (bbox[1] + bbox[3]) / 2 / image_height\n",
    "#     width = (bbox[2] - bbox[0]) / image_width\n",
    "#     height = (bbox[3] - bbox[1]) / image_height\n",
    "\n",
    "#     return f\"{class_id} {x_center} {y_center} {width} {height}\"\n",
    "\n",
    "# def split_video(video_path, xml_file, output_image_directory, output_annotation_directory, frame_rate=1, class_ids=None):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     annotations = read_annotations(xml_file)\n",
    "\n",
    "#     frame_number = 0\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         if frame_number in annotations:\n",
    "#             image_height, image_width, _ = frame.shape\n",
    "\n",
    "#             # Save the frame as an image\n",
    "#             output_image_file = os.path.join(output_image_directory, f\"frame_{frame_number:04d}.jpg\")\n",
    "#             cv2.imwrite(output_image_file, frame)\n",
    "\n",
    "#             # Save YOLO format annotation to file\n",
    "#             annotation_file_path = os.path.join(output_annotation_directory, f\"frame_{frame_number:04d}.txt\")\n",
    "#             with open(annotation_file_path, 'w') as yolo_file:\n",
    "#                 for annotation in annotations[frame_number]:\n",
    "#                     xmin, ymin, xmax, ymax = annotation['bbox']\n",
    "#                     label = annotation['label']\n",
    "\n",
    "#                     class_id = class_ids.get(label)\n",
    "#                     if class_id is not None:\n",
    "#                         yolo_format = convert_to_yolo_format(class_id, image_width, image_height, (xmin, ymin, xmax, ymax))\n",
    "#                         yolo_file.write(f\"{yolo_format}\\n\")\n",
    "\n",
    "#         # Create the output directories if they don't exist\n",
    "#         os.makedirs(output_image_directory, exist_ok=True)\n",
    "#         os.makedirs(output_annotation_directory, exist_ok=True)\n",
    "\n",
    "#         # Save the frame if it's within the desired frame rate\n",
    "#         if frame_number % int(cap.get(cv2.CAP_PROP_FPS) / frame_rate) == 0:\n",
    "#             frame_number += 1\n",
    "#             continue\n",
    "\n",
    "#         frame_number += 1\n",
    "\n",
    "#     cap.release()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     video_directory = \"C:/Users/user/Downloads/FYP/fypfiles/mbgdataset/flightavi\"\n",
    "#     annotation_directory = \"C:/Users/user/Downloads/FYP/fypfiles/mbgdataset/flightann\"\n",
    "#     output_image_directory = \"datasets/imageframes\"\n",
    "#     output_annotation_directory = \"datasets/labelframes\"\n",
    "#     frame_rate = 1\n",
    "\n",
    "#     class_ids = {\"tire\": 0, \"bottle\": 1, \"your_class_2\": 2, \"your_class_3\": 3, \"your_class_4\": 4, \"your_class_5\": 5}\n",
    "\n",
    "#     video_files = [f for f in os.listdir(video_directory) if f.endswith(\".avi\")]\n",
    "\n",
    "#     for video_file in video_files:\n",
    "#         video_path = os.path.join(video_directory, video_file)\n",
    "#         xml_file = os.path.join(annotation_directory, f\"{os.path.splitext(video_file)[0]}.xml\")\n",
    "#         split_video(video_path, xml_file, output_image_directory, output_annotation_directory, frame_rate, class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train, test, val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size (60%): 4533\n",
      "Validation set size (20%): 1511\n",
      "Test set size (20%): 1511\n",
      "\n",
      "Example entries in the training set:\n",
      "Image: frame_3559.jpg Label: frame_3559.txt\n",
      "Image: frame_5669.jpg Label: frame_5669.txt\n",
      "Image: frame_6310.jpg Label: frame_6310.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to the dataset directory\n",
    "dataset_directory = \"C:/Users/user/Downloads/projects/MBGprocess/\"\n",
    "\n",
    "# Set the paths for image and label folders\n",
    "image_folder = os.path.join(dataset_directory, \"imageframes\")\n",
    "label_folder = os.path.join(dataset_directory, \"labelframes\")\n",
    "\n",
    "# Get lists of image and label files\n",
    "image_files = os.listdir(image_folder)\n",
    "label_files = os.listdir(label_folder)\n",
    "\n",
    "# Ensure the lists are sorted for consistency\n",
    "image_files.sort()\n",
    "label_files.sort()\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "image_train, image_temp, label_train, label_temp = train_test_split(image_files, label_files, test_size=0.4, random_state=1)\n",
    "image_val, image_test, label_val, label_test = train_test_split(image_temp, label_temp, test_size=0.5, random_state=1)\n",
    "\n",
    "#First split to get 20% as test set\n",
    "image_train, image_test, label_train, label_test = train_test_split(image_files, label_files, test_size=0.2, random_state=1)\n",
    "#Split the training set again to get the validation set (requires calculation the get the needed percentage)\n",
    "image_train, image_val, label_train, label_val = train_test_split(image_train, label_train, test_size=0.25, random_state=1)\n",
    "\n",
    "# Display the lengths of the sets\n",
    "print(\"Train set size (60%):\", len(image_train))\n",
    "print(\"Validation set size (20%):\", len(image_val))\n",
    "print(\"Test set size (20%):\", len(image_test))\n",
    "\n",
    "# Now you have variables containing the file paths for each set\n",
    "train_set = list(zip(image_train, label_train))\n",
    "val_set = list(zip(image_val, label_val))\n",
    "test_set = list(zip(image_test, label_test))\n",
    "\n",
    "# Example: Print the first 5 entries in the training set\n",
    "print(\"\")\n",
    "print(\"Example entries in the training set:\")\n",
    "for i in range(3):\n",
    "    print(\"Image:\", train_set[i][0], \"Label:\", train_set[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\projects\\AutoYOLOv5\\yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\projects\\AutoYOLOv5\\myenv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt comet_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and val folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directing splitted images into their belonged folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s on MBG dataset\n",
    "!python train.py --img 512 --batch 32 --epochs 30 --data mbg_test.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python311\\python.exe: can't open file 'c:\\\\Users\\\\user\\\\Downloads\\\\projects\\\\AutoYOLOv5\\\\detect.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --source runs/train/exp/test1.jpg --weights best.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
